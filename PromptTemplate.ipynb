{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTemplate\n",
    "- PromptTemplate(https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html#langchain_core.prompts.prompt.PromptTemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='\\n        あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。\\n        質問: 日本の首都はどこですか？\\n    '\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "        あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。\n",
    "        質問: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# プロンプトを生成して表示\n",
    "prompt_value = prompt.invoke({\"question\": \"日本の首都はどこですか？\"})\n",
    "print(prompt_value) # プロンプトが生成された。実際にはこのプロンプトをLLMに送る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate\n",
    "`PromptTemplate`を会話形式を前提としたチャットモデル(e.g. `Chat Completions API`)に対応させたもの\n",
    "\n",
    "- `ChatPromptTemplate`(https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html#langchain_core.prompts.chat.ChatPromptTemplate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from_messages()\n",
    "明確にロールを分けて会話を再現したい時\n",
    "  - https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html#langchain_core.prompts.chat.ChatPromptTemplate.from_messages\n",
    "\n",
    "リストの`tuple`に設定したKeyに応じて、`SystemMessage`や`HumanMessage`(`BaseMessage`を継承)などのインスタンスとしてメッセージが生成される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。', additional_kwargs={}, response_metadata={}), HumanMessage(content='日本の首都はどこですか？', additional_kwargs={}, response_metadata={}), AIMessage(content='東京です。', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。\"), # tupleとしてメッセージを定義 -> SystemMessageとして扱われる\n",
    "        (\"human\", \"{question}\"),  # -> HumanMessageとして扱われる\n",
    "        (\"ai\", \"東京です。\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# プロンプトを生成して表示\n",
    "prompt_value = prompt.invoke({\"question\": \"日本の首都はどこですか？\"})\n",
    "print(prompt_value) # プロンプトが生成された。実際にはこのプロンプトをLLMに送る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseMessageを継承したクラスを使ってメッセージを定義するパターン\n",
    "これも同じ意味"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。', additional_kwargs={}, response_metadata={}), HumanMessage(content='{question}', additional_kwargs={}, response_metadata={}), AIMessage(content='東京です。', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。\"),\n",
    "        HumanMessage(content=\"{question}\"),\n",
    "        AIMessage(content=\"東京です。\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# プロンプトを生成して表示\n",
    "prompt_value = prompt.invoke({\"question\": \"日本の首都はどこですか？\"})\n",
    "print(prompt_value) # プロンプトが生成された。実際にはこのプロンプトをLLMに送る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from_template()\n",
    "シンプルにプロンプトを生成したい時\n",
    "- https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html#langchain_core.prompts.chat.ChatPromptTemplate.from_template\n",
    "\n",
    "`from_template()`を使うと全て`HumanMessage`として扱われる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='\\n    system: あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。\\n    human: 日本の首都はどこですか？\\n    ', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    system: あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。\n",
    "    human: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# プロンプトを生成して表示\n",
    "prompt_value = prompt.invoke({\"question\": \"日本の首都はどこですか？\"})\n",
    "print(prompt_value) # プロンプトが生成された。実際にはこのプロンプトをLLMに送る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MessagePlaceHolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。', additional_kwargs={}, response_metadata={}), HumanMessage(content='こんにちは！私はジョンと言います。', additional_kwargs={}, response_metadata={}), AIMessage(content='こんにちは！ジョンさん。どのようにお手伝いできますか？', additional_kwargs={}, response_metadata={}), HumanMessage(content='私の名前が分かりますか？', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。\"),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_value = prompt.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"こんにちは！私はジョンと言います。\"),\n",
    "            AIMessage(content=\"こんにちは！ジョンさん。どのようにお手伝いできますか？\"),\n",
    "        ],\n",
    "        \"input\": \"私の名前が分かりますか？\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(prompt_value) \n",
    "\n",
    "# for message in prompt_value.messages:\n",
    "#     print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Placeholder`を含める場合は`messages`はtupleのリスト(=`ChatPromptTemplate`型)である必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。', additional_kwargs={}, response_metadata={}), HumanMessage(content='日本の首都はどこですか？', additional_kwargs={}, response_metadata={}), AIMessage(content='東京です。', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。\"),\n",
    "        (\"placeholder\", \"{input}\"),  # placeholderを指定した場合、valueはtupleのリスト(=ChatPromptTemplate型)である必要がある\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_value = prompt.invoke({\"input\": [(\"user\", \"日本の首都はどこですか？\"), (\"ai\", \"東京です。\")]})\n",
    "\n",
    "print(prompt_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PlaceHolder検証\n",
    "各ケースにおけるPlaceHolderの挙動を検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= Case1: 通常のテキスト(str)にPlaceholderを使用する場合 ===================\n",
      "\n",
      "あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。\n",
      "質問: 日本の首都はどこですか？\n",
      "\n",
      "================= Case2: ChatPromptTemplateでPlaceholderを使用する場合 ===================\n",
      "================= invoke前 ===================\n",
      "input_variables=['question'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]\n",
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "================= invoke後 ===================\n",
      "messages=[SystemMessage(content='あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"{'日本の首都はどこですか？'}\", additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "print(\"================= Case1: 通常のテキスト(str)にPlaceholderを使用する場合 ===================\")\n",
    "# Case1: 通常のテキスト内でPlaceholderを使用する場合\n",
    "question = \"日本の首都はどこですか？\"\n",
    "\n",
    "text_prompt = f\"\"\"\n",
    "あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。\n",
    "質問: {question}\n",
    "\"\"\"\n",
    "print(text_prompt) # {question}にquestionとして定義した文字列が埋め込まれる\n",
    "\n",
    "print(\"================= Case2: ChatPromptTemplateでPlaceholderを使用する場合 ===================\")\n",
    "# Case2: ChatPromptTemplateでPlaceholderを使用する場合\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは優秀なアシスタントです。ユーザーからの質問に回答してください。\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"================= invoke前 ===================\")\n",
    "print(prompt) # 定義した時点ではPlaceholderが展開されない\n",
    "print(type(prompt))\n",
    "\n",
    "print(\"================= invoke後 ===================\")\n",
    "# プロンプトを生成して表示\n",
    "prompt_value = prompt.invoke({f\"question\": {question}})  # invokeを実行するとPlaceholderが展開される\n",
    "print(prompt_value)\n",
    "print(type(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChainStudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
